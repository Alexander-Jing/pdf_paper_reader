import os
import json
import csv
import re
import concurrent.futures
import requests
import time
from extract_pdf import *

# ====== é…ç½®åŒºåŸŸ ======
API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"  # æ›¿æ¢ä¸ºå®é™…APIåœ°å€
API_KEY = ""  # æ›¿æ¢ä¸ºAPIå¯†é’¥
PDF_FOLDER = "./papers"  # PDFæ–‡ä»¶å¤¹è·¯å¾„
OUTPUT_CSV = "results.csv"  # è¾“å‡ºCSVæ–‡ä»¶å
MAX_CONCURRENT = 4  # æœ€å¤§å¹¶å‘æ•°ï¼ˆæ ¹æ®APIé™åˆ¶è°ƒæ•´ï¼‰
YOUR_PAPER_TITLE = ""  # è¢«å¼•ç”¨çš„è®ºæ–‡æ ‡é¢˜
# =====================

def call_llm_api(pdf_text: str) -> dict:
    """
    è°ƒç”¨ç¡…åŸºæµåŠ¨APIåˆ†æPDFå†…å®¹
    ä¼˜åŒ–ç‚¹ï¼š
    1. ä½¿ç”¨å®˜æ–¹æ¨èæ¨¡å‹
    2. æ›´å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶
    3. è¯·æ±‚å‚æ•°ä¼˜åŒ–
    4. å“åº”è§£æç®€åŒ–
    """
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    
    # å¢å¼ºæç¤ºè¯è®¾è®¡ï¼ˆèšç„¦ç›®æ ‡è®ºæ–‡å¼•ç”¨ï¼‰
    prompt = f"""è¯·ä¸¥æ ¼åˆ†æå½“å‰PDFæ–‡ä»¶å†…å®¹ï¼Œå½“å‰è¾“å…¥çš„pdfæ–‡ä»¶å†…å®¹æ˜¯ä¸€ç¯‡å­¦æœ¯è®ºæ–‡ï¼š
    ## æ ¸å¿ƒä»»åŠ¡
    1. åœ¨è®ºæ–‡æ­£æ–‡ä¸­å®šä½å¯¹å‚è€ƒæ–‡çŒ®ã€Š{YOUR_PAPER_TITLE}ã€‹çš„å¼•ç”¨è¯„ä»·ï¼ˆâœ…æ­£é¢/âš ï¸ä¸­æ€§/âŒè´Ÿé¢ï¼‰
    2. æå–æ–½å¼•æ–‡çŒ®çš„æ ‡é¢˜å’ŒæœŸåˆŠåç§°
    3. æå–æ‰€æœ‰ä½œè€…å§“å(æŒ‰æ ‡å‡†æ ¼å¼: Zhang et al. æˆ– Li, Y.)

    ## è¾“å‡ºè¦æ±‚
    è¿”å›çº¯å‡€JSONå¯¹è±¡ï¼š
    {{
        "title": "æ–½å¼•æ–‡çŒ®æ ‡é¢˜",
        "journal": "æœŸåˆŠåç§°",
        "authors": ["ä½œè€…1", "ä½œè€…2"]ï¼Œå¦‚æœæ˜¯ä¸­æ–‡åï¼Œè¯·ç›´æ¥ä½¿ç”¨ä¸­æ–‡å,
        "citations": [{{
            "quote": "å¯»æ‰¾è®ºæ–‡æ­£æ–‡ä¸­å¯¹å‚è€ƒæ–‡çŒ®ã€Š{YOUR_PAPER_TITLE}ã€‹çš„å¼•ç”¨è¯„ä»·åŸæ–‡",
            "sentiment": "è¯„ä»·æ€§è´¨",
            "page": "é¡µç "
        }}]
    }}

    ## å¤„ç†è§„åˆ™
    - å¿½ç•¥éå¼•ç”¨å†…å®¹çš„æ–‡æœ¬
    - æ— ç›¸å…³å¼•ç”¨æ—¶è¿”å›ç©ºåˆ—è¡¨
    - ä¿æŒJSONç»“æ„å®Œæ•´

    --- PDFå†…å®¹(å‰30000å­—ç¬¦) ---
    {pdf_text}"""
    
    payload = {
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",  # å®˜æ–¹æ¨èæ»¡è¡€ç‰ˆæ¨¡å‹
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,  # ä½éšæœºæ€§ä¿è¯ç¨³å®šæ€§
        "max_tokens": 2000,   # å¢åŠ tokené™é¢ç¡®ä¿å®Œæ•´è¾“å‡º
        "response_format": {"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
    }
    
    max_retries = 3
    wait_time = 5  # åˆå§‹ç­‰å¾…æ—¶é—´(ç§’)
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                API_ENDPOINT, 
                headers=headers, 
                json=payload, 
                timeout=60
            )
            
            # æˆåŠŸå“åº”å¤„ç†
            if response.status_code == 200:
                response_data = response.json()
                try:
                    # ç›´æ¥æå–æ¨¡å‹è¿”å›çš„JSONå†…å®¹
                    content = response_data["choices"][0]["message"]["content"]
                    return json.loads(content)
                except (KeyError, json.JSONDecodeError) as e:
                    print(f"ğŸ“Š JSONè§£æé”™è¯¯: {str(e)}")
                    print(f"åŸå§‹å“åº”: {content}")
                    return {}
            
            # é™é€Ÿå¤„ç†ï¼ˆ429ï¼‰
            elif response.status_code == 429:
                print(f"â³ APIé™é€Ÿ(å°è¯• {attempt+1}/{max_retries}), ç­‰å¾…{wait_time}ç§’...")
                time.sleep(wait_time)
                wait_time *= 2  # æŒ‡æ•°é€€é¿ç­–ç•¥
            
            # æœåŠ¡ä¸å¯ç”¨ï¼ˆ503ï¼‰
            elif response.status_code == 503:
                print(f"ğŸ› ï¸ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨(å°è¯• {attempt+1}/{max_retries}), ç­‰å¾…{wait_time}ç§’...")
                time.sleep(wait_time)
                wait_time *= 2
            
            # å…¶ä»–APIé”™è¯¯
            else:
                print(f"âŒ APIé”™è¯¯({response.status_code}): {response.text[:200]}")
                return {}
                
        # ç½‘ç»œå¼‚å¸¸å¤„ç†
        except (requests.Timeout, requests.ConnectionError) as e:
            print(f"ğŸŒ ç½‘ç»œå¼‚å¸¸({attempt+1}/{max_retries}): {str(e)}")
            time.sleep(wait_time)
            wait_time *= 2
        
        # å…¶ä»–å¼‚å¸¸å¤„ç†
        except Exception as e:
            print(f"âš ï¸ æœªé¢„æœŸé”™è¯¯: {str(e)}")
            return {}
    
    print(f"ğŸš« è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})ï¼Œæ”¾å¼ƒè¯·æ±‚")
    return {}

def process_single_pdf(pdf_path: str) -> dict:
    """ç‹¬ç«‹å¤„ç†å•ä»½PDFçš„å…¨æµç¨‹"""
    filename = os.path.basename(pdf_path)
    print(f"ğŸ” å¼€å§‹å¤„ç†: {filename}")
    
    # æ­¥éª¤1ï¼šç‹¬ç«‹æå–æ–‡æœ¬ï¼ˆéš”ç¦»å…¶ä»–æ–‡ä»¶ï¼‰
    pdf_text = extract_pdf(pdf_path)
    if not pdf_text:
        return {"filename": filename, "error": "TEXT_EXTRACTION_FAILED"}
    
    # æ­¥éª¤2ï¼šç‹¬ç«‹è°ƒç”¨APIï¼ˆç¡®ä¿æ— ä¸Šä¸‹æ–‡å¹²æ‰°ï¼‰
    result = call_llm_api(pdf_text)
    if not result:
        return {"filename": filename, "error": "API_CALL_FAILED"}
    
    # è¿”å›ç»“æ„åŒ–ç»“æœ
    return {
        "filename": filename,
        "title": result.get("title", ""),
        "journal": result.get("journal", ""),
        "authors": result.get("authors", []),
        "citations": result.get("citations", [])
    }

def main():
    # è·å–æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = [
        os.path.join(PDF_FOLDER, f) 
        for f in os.listdir(PDF_FOLDER) 
        if f.lower().endswith(".pdf")
    ]
    print(f"ğŸ“‚ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ | å¹¶å‘æ•°: {MAX_CONCURRENT}")
    
    # å¹¶å‘å¤„ç†ï¼ˆæ¯ä¸ªPDFç‹¬ç«‹è°ƒç”¨ï¼‰[8](@ref)
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT) as executor:
        future_to_file = {executor.submit(process_single_pdf, pdf): pdf for pdf in pdf_files}
        for future in concurrent.futures.as_completed(future_to_file):
            pdf_path = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                cite_count = len(result.get("citations", []))
                status = f"âœ… å‘ç°{cite_count}å¤„å¼•ç”¨" if cite_count > 0 else "âš ï¸ æœªæ‰¾åˆ°å¼•ç”¨"
                print(f"{status} | {os.path.basename(pdf_path)}")
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {os.path.basename(pdf_path)} - {str(e)}")
    
    # ä¿å­˜CSVç»“æœ[2](@ref)
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["æ–‡ä»¶å", "æ–½å¼•æ ‡é¢˜", "æœŸåˆŠ", "ä½œè€…", "å¼•ç”¨åŸæ–‡", "æƒ…æ„Ÿ", "é¡µç "])
        for res in results:
            for cite in res.get("citations", []):
                writer.writerow([
                    res["filename"],
                    res.get("title", ""),
                    res.get("journal", ""),
                    "; ".join(res.get("authors", [])),
                    cite.get("quote", "")[:200] + "..." if len(cite.get("quote", "")) > 200 else cite.get("quote", ""),
                    cite.get("sentiment", ""),
                    cite.get("page", "")
                ])
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
